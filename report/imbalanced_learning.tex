\section{Imbalanced Learning}
We then proceeded with imbalance learning task, we used the same variable \texttt{averageRating} to categorize the data into three classes:
low (0-4], medium (4-7], high (7-10]. The distribution of the classes is 4979, 67394, 72539 respectively, where the low class accounts for 
approx 3.5\% of the dataset, which is highly imbalanced.
We then applied Decision Tree classifier to the initial dataset, to be
used as a baseline for other models.
Training was performed using \texttt{RandomSearchCV} to tune the
hyperparameters of the Decision Tree Classifier.

Table~\ref{tab:clean} reports the performance of the model.


\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Model Name} & \textbf{Accuracy} & \textbf{Precision (on Low Class)} & \textbf{Recall (on Low Class)} \\ \hline
    DT & 0.63 & 0.18 & 0.19 \\ \hline
  \end{tabular}
  \caption{Plain Decision Tree Model Performance}
  \label{tab:clean}
\end{table}

For the next subsections, only the training set was changed.

\subsection{Oversampling}
In the oversampling techniques, we applied \texttt{Synthetic Minority
Oversampling Technique (SMOTE)} and \texttt{Adaptive Synthetic
Sampling (ADASYN)} to increase the size of the minority class.
Table~\ref{tab:over} provides the performances of the oversampling
methods.

Despite the different methodologies, both approaches resulted in
similar overall performance, with SMOTE achieving 0.63 accuracy and
0.23 recall for the low class, and ADASYN achieving 0.62 accuracy and
0.21 recall.
This indicates that while the oversampling strategy can influence
minority samples representation, it may also introduce synthetic
noise, limiting overall accuracy improvements.

Comparing the oversampling techniques with the baseline, we observe
that both \texttt{SMOTE} and \texttt{ADASYN} improve
recall for the low class, indicating that oversampling
helps the model detect more minority-class instances.  

However, this improvement in recall comes with a slight reduction in
precision. This suggests that while oversampling effectively
increases sensitivity to the minority class, it may introduce additional
false positives and does not substantially improve overall model
performance.


\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|l|l|}
  \hline
  \textbf{Model Name} & \textbf{Accuracy} & \textbf{Precision (on Low Class)} & \textbf{Recall (on Low Class)} \\ \hline
  SMOTE & 0.63 & 0.16 & 0.23 \\ \hline
  ADASYN & 0.62 & 0.15 & 0.21 \\ \hline
  \end{tabular}
  \caption{Oversampling Models' Performances}
  \label{tab:over}
\end{table}



\subsection{Undersampling}

In the undersampling techniques, we applied \texttt{RandomUnderSampler},
\texttt{Edited Nearest Neighbors (ENN)} and
clustering-based undersampling using \texttt{DBSCAN} combined with
\texttt{K-Means}.
Table~\ref{tab:under} provides the performances of the undersampling
models.

\texttt{RandomUnderSampler} alone achieved high recall with
moderate accuracy, though its precision was very low, indicating
that it was too aggressive.
The model was found to perform slightly better when using \texttt{ENN}
with \texttt{kind\_sel=all} and \texttt{n\_neighbors=1}, meaning that
a more conservative approach proved to be more beneficial.

The clustering-based approach (\texttt{DBSCAN + K-Means}) achieved an
overall balanced performance. This setup was chosen because
it allowed to remove noise points to address k-means' weakness.

These results highlight the trade-off inherent in undersampling:
methods that aggressively reduce majority samples improve minority
class detection but can substantially reduce overall accuracy.


\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|l|l|}
  \hline
  \textbf{Model Name} & \textbf{Accuracy} & \textbf{Precision (on Low Class)} & \textbf{Recall (on Low Class)} \\ \hline
  RandomUnderSampler & 0.63 & 0.09 & 0.71 \\ \hline
  ENN & 0.68 & 0.20 & 0.23 \\ \hline
  DBSCAN + K-Means & 0.56 & 0.10 & 0.57 \\ \hline
  \end{tabular}
  \caption{Undersampling Models' Performances}
  \label{tab:under}
\end{table}


\subsection{Conclusion}

As shown in Tables~\ref{tab:clean}, \ref{tab:over}, and \ref{tab:under},
handling class imbalance significantly affects model performance,
particularly for the minority (low) class.

Oversampling methods
(\texttt{SMOTE} and \texttt{ADASYN}) improved recall compared to the
baseline Decision Tree, allowing better detection of minority instances,
but at the cost of slightly reduced precision and no substantial gain
in overall accuracy.

Undersampling approaches produced more varied trade-offs.
\texttt{RandomUnderSampler} achieved the highest recall but very
low precision, whereas \texttt{ENN} increased overall accuracy
with only modest improvement in recall.
The clustering-based \texttt{DBSCAN + K-Means} approach provided a
more balanced solution, while sacrificing accuracy and recall. 

Overall, no single method simultaneously optimizes all metrics.
The choice of strategy should therefore be guided by the specific task
priorities.
