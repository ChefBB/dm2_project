\section{Advanced Classification}
\label{sec:advanced_classification}


In this section, we present the classification results for two target variables: 
\texttt{averageRating} (binned into six classes) and \texttt{titleType} (six classes). 
All models were trained using the preprocessed dataset described in section~\ref{sec:data_preparation}.

The first target variable, \texttt{titleType}, originally contained several categories with 
highly imbalanced frequencies. In particular, \texttt{tvSpecial} and \texttt{video} were strongly 
underrepresented, while entries labeled as \texttt{videoGame} were removed due to insufficient sample size. 
To obtain more meaningful and stable classes, related categories were merged according to the 
following mapping: \texttt{movie} and \texttt{tvMovie} $\rightarrow$ \texttt{movie}; \texttt{short} and 
\texttt{tvShort} $\rightarrow$ \texttt{short}; \texttt{tvSeries} and 
\texttt{tvMiniSeries} $\rightarrow$ \texttt{tvSeries}; while \texttt{tvEpisode}, \texttt{tvSpecial},
 and \texttt{video} remained unchanged. 
 The attribute \texttt{canHaveEpisodes} was removed to avoid data leakage, 
 as it directly reveals information about the target \texttt{titleType}.

The second target variable, \texttt{averageRating}, 
was transformed into six discrete classes as described in section~\ref{sec:outlier} to convert the continuous target 
into a multi-class problem.



%-------------------------------------------------------------------------------

\subsection{Logistic Regression}
\label{subsec:logistic}

\subsubsection{titleType task}
%-------------------------------------------------------------------------------
% titleType logistic
%-------------------------------------------------------------------------------
For the classification of the \texttt{titleType} variable, the target was transformed 
into numerical labels using \textit{LabelEncoder} and all numerical features were scaled 
with \texttt{StandardScaler} to ensure comparability across variables.

Since the problem is multi-class, we chose to present the results using the 
\textit{One-vs-Rest (OVR)} approach, which trains a binary classifier for each class. 
We also tested the \textit{multinomial} strategy, but it was significantly slower and 
produced comparable results, so OVR was selected for efficiency.

To optimize the model, a hyperparameter search was performed using 
\texttt{RandomizedSearchCV} with
\newline\texttt{StratifiedKFold} (5 folds), ensuring that the class 
distribution was preserved in each fold. The solver was set to \texttt{saga}, and the 
parameters tested included the regularization term $C$ (50 values logarithmically spaced 
between $10^{-3}$ and $10^2$), \texttt{penalty} (\texttt{l2} and \texttt{l1}), and 
\texttt{class\_weight} (\texttt{None} and \texttt{balanced}). The hyperparameter 
search was performed using \texttt{f1\_macro} as the scoring metric to balance 
performance across all classes. The best parameters selected by the search were 
C = 2.95, penalty = l1, and class\_weight = balanced.

For computational efficiency, the search was conducted on a 10\% stratified sample 
of the dataset, which was approximately representative of the original class distribution. The final 
model was then refitted on the full dataset using the best parameters. Evaluation on 
the test set was carried out using the confusion matrix, classification report, and 
one-vs-rest ROC curves for each class. The main performance metrics are summarized in 
Table~\ref{tab:logistic_report_t}, and the corresponding ROC curves are shown in 
Figure~\ref{fig:log_roc_curves_t}. 

\begin{figure}[ht]
    \centering
    \begin{minipage}{0.45\textwidth} 
    \centering
    % \captionof{table}{Classification report for \texttt{titleType}}
    % \label{tab:logistic_report_t}
    \small 
    \begin{tabular}{lccc}
    \hline
    \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score}\\
    \hline
    tvEpisode  & 0.90 & 0.88 & 0.89  \\
    movie      & 0.92 & 0.65 & 0.76  \\
    short      & 0.74 & 0.87 & 0.80  \\
    tvSeries   & 0.50 & 0.35 & 0.41  \\
    video      & 0.21 & 0.47 & 0.29  \\
    tvSpecial  & 0.07 & 0.52 & 0.12  \\
    \hline
    \textbf{Accuracy}    & \multicolumn{3}{c}{0.75} \\
    \textbf{Macro avg}   & 0.56 & 0.62 & 0.54  \\
    \textbf{Weighted avg}& 0.83 & 0.75 & 0.78  \\
    \hline
    \end{tabular}
    \captionof{table}{Classification report for \texttt{titleType}}
    \label{tab:logistic_report_t}
    \end{minipage}
    \hfill
    \begin{minipage}{0.4\textwidth} 
    \centering
    \includegraphics[width=\textwidth]{plotsss/log_roc_curves_t.png} 
    \captionof{figure}{ROC Logistic Regression}
    \label{fig:log_roc_curves_t} 
    \end{minipage}
    \end{figure}


From the results, we observe that the model achieves high performance for the 
\texttt{movie}, \texttt{short}, and \texttt{tvEpisode} classes, with F1-scores of 0.76, 
0.80, and 0.89, respectively. The model performs less well on \texttt{tvSeries}, 
\texttt{tvSpecial}, and \texttt{video}, likely due to lower support in the dataset.
Overall, the model reaches an accuracy of 0.75, 
a macro-average F1 of 0.54, and a weighted F1 of 0.78. These results highlight that 
while the model discriminates well among the more common classes, its performance is still
limited for rarer classes. 
This pattern is also reflected in the ROC curves shown in Figure~\ref{fig:log_roc_curves_t}, 
where the model clearly separates the more frequent classes, while discrimination is more limited 
for the less common categories.

% Coefficient analysis provided insight into the influence of each feature on the probability of belonging to different classes. 
% For each OVR classifier, the coefficients indicate how much a feature increases or decreases the probability of class membership 
% relative to the others. Globally, the most influential features were \texttt{budget}, \texttt{duration}, and \texttt{votes}. 
% Examining individual classes, for the \textit{film} class, \texttt{votes} and \texttt{budget} positively contribute, while 
% \texttt{duration} has a negative effect; for the \textit{series} class, \texttt{actors\_popularity} is positive, 
% while \texttt{budget} is negative; for the \textit{documentary} class, \texttt{duration} is positive, whereas
%  \texttt{votes} is negative. 
%  This interpretation was visualized through a coefficient heatmap and can also be summarized in a table of the top positive and negative features for each class.

Furthermore, coefficient analysis highlighted the main drivers for each class. Globally, \texttt{totalNomitations}, \texttt{numRegions}, and \texttt{runtimeMinutes} were most influential. 
For individual classes, for example, \texttt{movie} is positively influenced by \texttt{totalNomitations} but negatively by \texttt{genre3}; 
\texttt{short} is positively influenced by \texttt{totalNomitations} and negatively by \texttt{companiesNumber}; 
\texttt{tvEpisode} is positively influenced by \texttt{Europe} and negatively by \texttt{numRegions}.


%-------------------------------------------------------------------------------
% rating logistic 
%-------------------------------------------------------------------------------
% Ensure that \subsubsection is not placed inside an environment like figure or table
\subsubsection{averageRating task}
Building upon the approach described for \texttt{titleType}, we trained a 
Logistic Regression model for the multi-class \texttt{averageRating} variable. 
All numerical features were scaled and, in addition, the categorical variable 
\texttt{titleType} was included as a predictor and processed via \textit{One-Hot Encoding}.

Hyperparameters were optimized using again \texttt{RandomizedSearchCV} with \texttt{StratifiedKFold} (5 folds), 
performed on a 10\% stratified sample of the data for computational efficiency. 
The scoring metric was \texttt{f1\_macro}, and the search explored the same set of parameters used for \texttt{titleType}. 
The optimal configuration was found to be: C = 0.08, penalty = l2, and class\_weight = balanced.

The best parameters were then used to fit the model on the full dataset, and evaluation on the test set was carried out. 

% From the results, we observe that the model shows limited overall performance on the \texttt{rating\_class} test set,
% achieving an accuracy of 0.27, a macro-average F1 of 0.25, and a weighted F1 of 0.27. 
% Performance varies considerably across classes: the model predicts relatively better the mid-range ratings 
% (\texttt{[7, 8)}), with an F1-score of 0.40, while predictions for the lower and higher extremes 
% (\texttt{[1, 5)}, \texttt{[9, 10)}) are notably weaker. 


The results for the \texttt{averageRating} class are reported in 
Table~\ref{tab:logistic_report_rating} and Figure~\ref{fig:log_roc_curves_rating}. 
Overall, the model shows limited predictive ability 
(\textbf{accuracy = 0.27}, \textbf{macro F1 = 0.25}), with marked variability across classes. 
The extreme intervals, \texttt{[1,5)} and \texttt{[9,10)}, are detected with relatively high recall 
(0.44 and 0.57), but their low precision yields modest F1-scores. 
In contrast, the central ranges, which dominate the distribution, prove more difficult to classify: 
\texttt{[6,7)} reaches only 0.13 in F1, while \texttt{[7,8)} performs better at 0.40. 

Regarding ROC curves: discrimination is stronger for the extreme categories 
(AUCs of 0.75 and 0.76), whereas separability is weaker in the central intervals 
(\texttt{[6,7)}: 0.60, \texttt{[7,8)}: 0.64). 
This suggests that Logistic Regression tends to distinguishing the most polarized cases,
while it struggles to capture subtle differences in the middle of the rating scale.
 

\begin{figure}[ht]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        % \captionof{table}{Classification report for \texttt{rating\_class}}
        % \label{tab:logistic_report_rating}
        \small
        \begin{tabular}{lccc}
        \hline
        \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score}\\
        \hline
        \texttt{[1, 5)}   & 0.20 & 0.44 & 0.27 \\
        \texttt{[5, 6)}   & 0.26 & 0.32 & 0.29 \\
        \texttt{[6, 7)}   & 0.39 & 0.08 & 0.13 \\
        \texttt{[7, 8)}   & 0.48 & 0.34 & 0.40 \\
        \texttt{[8, 9)}   & 0.25 & 0.22 & 0.24 \\
        \texttt{[9, 10)}  & 0.09 & 0.57 & 0.16 \\
        \hline
        \textbf{Accuracy}    & \multicolumn{3}{c}{0.27} \\
        \textbf{Macro avg}   & 0.28 & 0.33 & 0.25 \\
        \textbf{Weighted avg}& 0.35 & 0.27 & 0.27 \\
        \hline
        \end{tabular}
        \captionof{table}{Classification report for \texttt{rating\_class}}
        \label{tab:logistic_report_rating}
        \end{minipage}
    \hfill
    \begin{minipage}{0.4\textwidth} 
    \centering
    \includegraphics[width=\textwidth]{plotsss/log_roc_curves_rating.png} 
    \captionof{figure}{ROC curves for \texttt{rating\_class} (OvR)}
    \label{fig:log_roc_curves_rating} 
    \end{minipage}
\end{figure}





%-------------------------------------------------------------------------------
\subsection{Support Vector Machines}
\label{subsec:svm}

%-------------------------------------------------------------------------------
% titleType
%-------------------------------------------------------------------------------
\subsubsection{titleType task}
We applied Support Vector Machines (SVM) to the \texttt{titleType} classification task.
Both linear and non-linear kernels were explored in order to evaluate how decision boundary complexity influences predictive performance.

The first experiment used a Linear SVM trained on the full dataset.  
A grid search with five-fold cross validation was carried out on the parameters 
$C \in \{0.01, 0.1, 1, 10, 100\}$ and $max\_iter \in \{1000, 5000, 10000\}$. 
The optimal configuration, with $C=100$ and $max\_iter=1000$, achieved a test accuracy of 0.81. 
While precision and recall were high for majority classes (\textit{movie}, \textit{short}, \textit{tvEpisode}), 
the classifier failed on \textit{tvSeries}, \textit{tvSpecial}, and \textit{video}, 
indicating that a linear decision boundary is insufficient for this problem.  

Non-linear kernels were then evaluated. 
A grid search was first performed on a stratified 10\% subset of the training set to efficiently explore a wide range of hyperparameters for each kernel, 
since a full search on the complete dataset would have been computationally prohibitive. 
All kernels were tested with $C$ ranging from 0.01 to 100 and $\gamma$ set to \texttt{scale} or \texttt{auto}.
For the polynomial kernel, the degree was varied between 2 and 4 and \texttt{coef0} set to 0 or 1.
For the sigmoid kernel, \texttt{coef0} was also explored at 0 and 1.
% $C$ was varied from 0.01 to 100 for all kernels.
% For the RBF kernel, $\gamma$ was set to \texttt{scale} or \texttt{auto}.
% The polynomial kernel was tested with degrees 2, 3 and 4, $\gamma$ as \texttt{scale} or \texttt{auto}, and \texttt{coef0} 0 or 1.
% The sigmoid kernel was explored with $\gamma$ \texttt{scale}/\texttt{auto} and \texttt{coef0} 0 or 1.

The best configuration for each kernel, reported in Table~\ref{tab:svm_results}, was then retrained on the full dataset and evaluated on the test set. 
Both RBF and polynomial kernels reached approximately 0.90 test accuracy, substantially outperforming the linear baseline and sigmoid. 
The RBF kernel was selected as the reference non-linear model due to slightly more stable results and improved recall on the under-represented classes.


% ROC curves were used to evaluate class separability 
% %(Figure~\ref{fig:roc_four}),
% showing excellent separation for majority classes, although minority categories remained problematic. 
% ???????????????????????????????????????????????????????????????
% ???????????????????????????????????????????????????????????????
% ???????????????????????????????????????????????????????????????

% \textcolor{red}{I will change the text and explain the figures better.}

To address class imbalance, the RBF kernel was retrained with \texttt{class\_weight=balanced}.
This model reached a slightly lower overall accuracy of 0.84, 
but recall for \textit{tvSpecial} and \textit{video} improved, providing a more equitable classification across categories.  
% Confusion matrices (Figure~\ref{fig:rbf_balanced_two}) 
% illustrate that \textit{tvSpecial} and \textit{video} ... 
The corresponding ROC curves (Figure~\ref{fig:rbf_balanced_two}b) show high separability for all classes, 
with AUC values of 0.99 for \textit{movie}, \textit{short}, and \textit{tvEpisode}, and 0.95 for \textit{tvSeries}, 
\textit{tvSpecial}, and \textit{video}, confirming that the balanced model achieves good discrimination 
for the minority categories too. 

Then an analysis of the support vectors was conducted.
In the unbalanced RBF, nearly all points of minority classes became support vectors, 
while in the balanced model the total number of support vectors increased but was more evenly distributed across classes, 
indicating a more complex but fairer decision function. 

Table~\ref{tab:svm_results} summarizes the main results, including the parameters used for each kernel and the corresponding test performance. 


\begin{table}[h]
\centering
\caption{Comparison of SVM models on the IMDb classification task.}
\label{tab:svm_results}
\begin{tabular}{lccc}
\hline
\textbf{Model} & \textbf{Best Params (main)} & \textbf{Test Accuracy} & \textbf{Macro F1-score} \\
\hline
Linear SVM & $C=100$, $max\_iter=1000$ & 0.81 & 0.45 \\
RBF kernel & $C=10$, $\gamma=\text{scale}$ & 0.90 & 0.64 \\
Polynomial kernel & $C=10$, degree=3, $\gamma=\text{auto}$ & 0.90 & 0.64 \\
Sigmoid kernel & $C=0.1$, $\gamma=\text{auto}$ & 0.65 & 0.36 \\
RBF (balanced) & $C=10$, $\gamma=\text{scale}$, balanced & 0.84 & 0.65 \\
\hline
\end{tabular}
\end{table}
    

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/new_cm_rbf_balanced.png}
        \caption{Confusion Matrix RBF balanced}
        \label{fig:cm_rbf_balanced}
    \end{subfigure}
    \begin{subfigure}[b]{0.43\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/new_roc_rbf_balanced.png}
        \caption{ROC RBF balanced}
        \label{fig:roc_rbf_balanced}
    \end{subfigure}
    \caption{Confusion Matrix and ROC Curve for the SVM (kernel rbf and balanced class weight) on the \texttt{titleType} classification task.}  
    \label{fig:rbf_balanced_two}
\end{figure}


\subsubsection{averageRating task}
Support Vector Machines were applied to the \texttt{averageRating} classification task using the same kernels and hyperparameter search strategies as for \texttt{titleType}.
A Linear SVM achieved only 0.37 accuracy and a macro F1-score of 0.28, indicating that a linear decision boundary is inadequate.

Non-linear kernels were then explored. Both RBF and polynomial kernels reached similar test accuracy around 0.39, substantially outperforming the sigmoid kernel (0.28).
Due to slightly more stable results and better handling of minority categories, the RBF kernel was selected as the reference model.

To address class imbalance, the RBF model was retrained with \texttt{class\_weight=balanced}.
Overall accuracy decreased slightly to 0.29, but recall improved for the minority classes, particularly the extreme bins $[1,5)$ and $[9,10)$.
The classification report shows that the model captures a high proportion of extreme instances (recall 0.55 for $[1,5)$ and 0.63 for $[9,10)$), although precision is low (0.23 and 0.10, respectively), resulting in moderate F1-scores (0.33 and 0.18).
Intermediate bins are predicted with higher precision but lower recall, reflecting the difficulty of separating densely populated central categories.

ROC curves

confirm this pattern: AUC is highest for the extremes ($[1,5)$: 0.79, $[9,10)$: 0.80) and lower for intermediate bins ($[6,7)$: 0.61, $[7,8)$: 0.65), indicating that the model distinguishes extreme ratings better than the mid-range values.

Overall, non-linear kernels improve performance over linear SVM, but the ordinal nature of \texttt{averageRating} and the strong class imbalance continue to challenge accurate prediction across all bins.

\subsection{Ensemble methods}

\subsubsection{titleType task}

\begin{table}[H]
    \centering
    \begin{minipage}{0.55\textwidth}
        Table \ref{tab:ensemble_param_titletype} shows the best hyperparameter
        configurations found for Random Forest and AdaBoost on the
        \texttt{titleType} task.\\

        For Random Forest, the trees are quite deep and complex,
        with low minimum samples per split and leaf.\\

        For AdaBoost, the learning rate is very low, leading to
        slow, incremental learning.
        The base estimator is a decision tree with
        considerable depth and higher minimum samples per split and
        leaf than Random Forest's trees, indicating that each weak
        learner is slightly less complex.\\

        Again, the two models use a very different number of
        estimators, with Random Forest employing a much larger
        ensemble. For feature importances, both models assign
        over half of the importance to \texttt{runtimeMinutes},
        indicating a high dependence on this feature for
        classification.

    \end{minipage}
    \hfill
    \begin{minipage}{0.4\textwidth}
        \centering
        \begin{tabular}{lc}
        \hline
        \multicolumn{2}{c}{\textbf{Random Forest}} \\
        \hline
        \texttt{n\_estimators} & 42 \\
        \texttt{max\_depth} & 19 \\
        \texttt{min\_samples\_split} & 4 \\
        \texttt{min\_samples\_leaf} & 3 \\
        \texttt{max\_features} & 0.74 \\
        \texttt{criterion} & gini \\
        \texttt{class\_weight} & \texttt{None} \\
        \hline
        \multicolumn{2}{c}{\textbf{AdaBoost}} \\
        \hline
        \texttt{n\_estimators} & 11 \\
        \texttt{learning\_rate} & 0.03 \\
        \texttt{estimator\_\_max\_depth} & 14 \\
        \texttt{estimator\_\_min\_samples\_split} & 18 \\
        \texttt{estimator\_\_min\_samples\_leaf} & 10 \\
        \hline
        \end{tabular}
        \caption{Hyperparameter search space for ensemble models.}
        \label{tab:ensemble_param_titletype}
    \end{minipage}
    
\end{table}

Table~\ref{tab:classification_report} summarizes the classification
report for both models.
\begin{table}[H]
    \centering
    \begin{tabular}{lccccc}
    \hline
    \textbf{Model} & \textbf{Accuracy} & \textbf{Macro Precision} & \textbf{Macro Recall} & \textbf{Macro F1-score} \\
    \hline
    Random Forest & 0.92 & 0.84 & 0.71 & 0.75 \\
    AdaBoost & 0.92 & 0.81 & 0.70 & 0.73 \\
    \hline
    \end{tabular}
    \caption{Performances of the models on the \texttt{titleType} classification task.}
    \label{tab:classification_report}
\end{table}


Figures~\ref{fig:cm_rf_titletype} shows the confusion matrix
for Random Forest.
Consistently good performances can be observed across the most
represented classes. \texttt{video} and \texttt{tvSpecial} are the
classes that cause the most problems, since they are often
misclassified as the more frequent classes. The class \texttt{movie}
attracts most of these misclassifications, likely due to
similar durations of the products in these categories.
Likely due to the same reason, \texttt{video} is also often
misclassified as \texttt{short}.\\

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/conf_matr_rf_titletype}
        \caption{Confusion Matrix - Random Forest}
        \label{fig:cm_rf_titletype}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.50\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/roc_rf_titletype}
        \caption{ROC Curve - Random Forest}
        \label{fig:roc_rf}
    \end{subfigure}
    \caption{Confusion matrix and ROC Curve for Random Forest on the \texttt{titleType} classification task.}
    \label{fig:cm_comparison}
\end{figure}

The ROC curve shows high AUCs for all classes, with the lowest ones
being for the aformentioned less represented classes, both
achieving 0.96. This indicates that the model is very good at
maintaining a low false positive rate for these, while
correctly identifying the more represented classes.

\subsubsection{averageRating task}
\begin{table}[H]
    \centering
    \begin{minipage}{0.55\textwidth}
        AdaBoost and Random Forest models were trained on the
        classification tasks, while being optimized via Stratified
        Randomized Search with 5-fold
        cross-validation over a predefined hyperparameter space.\\

        Table~\ref{tab:ensemble_param} shows the best
        hyperparameters found for Random Forest and AdaBoost on the
        \texttt{averageRating} task.\\

        For Random Forest, a relatively high maximum depth as well
        as low values for minimum samples per split and leaf
        indicate that the individual trees are quite complex.\\

        For AdaBoost, the base estimator is again characterized by
        high complexity. The learning rate is moderate, leading to
        reasonably fast learning.
    \end{minipage}
    \hfill
    \begin{minipage}{0.4\textwidth}
        \centering
        \begin{tabular}{lc}
        \hline
        \multicolumn{2}{c}{\textbf{Random Forest}} \\
        \hline
        \texttt{n\_estimators} & 97 \\
        \texttt{max\_depth} & 19 \\
        \texttt{min\_samples\_split} & 2 \\
        \texttt{min\_samples\_leaf} & 6 \\
        \texttt{max\_features} & 0.91 \\
        \texttt{criterion} & gini \\
        \texttt{class\_weight} & \texttt{None} \\
        \hline
        \multicolumn{2}{c}{\textbf{AdaBoost}} \\
        \hline
        \texttt{n\_estimators} & 67 \\
        \texttt{learning\_rate} & 0.45 \\
        \texttt{estimator\_\_max\_depth} & 17 \\
        \texttt{estimator\_\_min\_samples\_split} & 7 \\
        \texttt{estimator\_\_min\_samples\_leaf} & 1 \\
        \hline
        \end{tabular}
        \caption{Hyperparameters found for ensemble models.}
        \label{tab:ensemble_param}
    \end{minipage}
\end{table}
The two models use a different number of estimators,
with Random Forest employing a larger ensemble.
This is likely due to the fact that Random Forest builds
trees independently, while AdaBoost adds weak learners
sequentially, focusing on correcting previous errors.

Another interesting aspect is the distribution of feature importances.
Both models assign about 50\% of the importance to the same set of
features (\texttt{ratingCount}, \texttt{startYear},
\texttt{runtimeMinutes} and \texttt{deltaCredits}).

Random Forest assigns about 50\% of the total importance to these
features, spreading it evenly among them.
AdaBoost assigns about 75\%, 41 of which is
attributed to \texttt{ratingCount} and \texttt{startYear}.



Table~\ref{tab:report_ensemble} summarizes the
classification report for both models.
Although the performances of the two models are similar,
Random Forest outperforms AdaBoost in all metrics but Macro-averaged
Precision. The weighted averaged metrics, here not reported,
also give the edge to Random Forest.

\begin{table}[H]
    \centering
    \begin{tabular}{lccccc}
    \hline
    \textbf{Model} & \textbf{Accuracy} & \textbf{Macro Precision} & \textbf{Macro Recall} & \textbf{Macro F1-score} \\
    \hline
    Random Forest & 0.45 & 0.47 & 0.35 & 0.37 \\
    AdaBoost & 0.43 & 0.49 & 0.33 & 0.35 \\
    \hline
    \end{tabular}
    \caption{Performances of the models on the \texttt{averageRating} classification task.}
    \label{tab:report_ensemble}
\end{table}
Performances of both models are limited, and the results show that
the different classes are not well separated. This can be observed
in Random Forest's confusion matrix in figure~\ref{fig:cm_rf}.
The matrix also shows that many of the misclassifications occur
within adjacent classes, which is expected given the ordinal
nature of the target variable.
In particular, a lot of confusion occurs between the most
represented classes, \texttt{[6,7)}, \texttt{[7,8)} and
\texttt{[8,9)}. This is likely due to the fact that these classes
cover a wide variety of titles that perform similarly in terms
of ratings. Additionally, as seen in graph~\ref{fig:rating_dist},
Many of the titles have ratings that fall close to the
boundaries between these classes, making it more difficult for the
model to distinguish between them.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.44\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/conf_matr_rf_rating.png}
        \caption{Confusion Matrix - Random Forest}
        \label{fig:cm_rf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.52\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/roc_rf_rating.png}
        \caption{ROC Curve - Random Forest}
        \label{fig:roc_rating}
    \end{subfigure}
    \caption{Confusion matrix and ROC Curve for Random Forest on the \texttt{averageRating} classification task.}
    \label{fig:cm_roc_rating}
\end{figure}
Figure~\ref{fig:roc_rating} shows the ROC curve for AdaBoost.
% Min AUC: 70
AUCs are generally high, with the lowest ones being 0.70 and 0.72,
although these correspond to the highest-represented classes.
This suggests that the models are better at separating the
extreme classes, while the more frequent
intermediate ones are more difficult to distinguish.


%-------------------------------------------------------------------------------
% titleType ensemble
%-------------------------------------------------------------------------------



\subsection{Neural Networks}

For both classification tasks, a feedforward neural network was
implemented.
In both cases, the train split was 60\%, the validation split
20\% and the test split 20\%. For both tasks, these splits
were stratified according to the target variable.

% -------------------------------------------------------------------------------
% titletype task
% -------------------------------------------------------------------------------
\subsubsection{titleType task}
The neural network's architecture for the \texttt{titleType}
classification task has three inputs:
\begin{itemize}
    \item \texttt{region}: handles the region feature, through
    a Dense layer with 8 units and ReLU activation;
    \item \texttt{genre}: handles the genre feature, through
    a Dense layer with 8 units and ReLU activation;
    \item \texttt{numerical}: handles the other numerical
    features, through a Dense layer with 112 units and ReLU
    activation.
\end{itemize}
The outputs of the three branches are then concatenated and
passed to two Dense layers with 64 and 32 units respectively,
each with ReLU activation.
The output layer has 6 units (one for each class) and
softmax activation.
For each Dense layer, Batch Normalization is applied,
to achieve better stability.
A dropout of 0.2 is also applied everywhere but in the input
branches.


The optimizer used was Adam with a learning rate of 0.0001,
and the loss function was Categorical Crossentropy.
The model was trained for a maximum of 500 epochs, with early
stopping based on validation loss with a patience of 20 epochs.
The batch size used was 64, and balanced class weights were used to
address class imbalance.

Figures~\ref{fig:nn_performance_titletype} shows the training and
validation loss and accuracy over epochs.
Both graphs show a tendency of training metrics to keep improving
in later epochs compared to validation, which instead starts
stabilizing much earlier. This is likely due to the low
value given to the learning rate, which allows the model
to learn finer patterns. In this case, these appear to be too
specific to the tranining set, though this doesn't seem to lead
to particular overfitting.


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/loss_titletype.png}
        % \caption{Loss - Neural Network}
        \label{fig:loss_nn_titletype}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/accuracy_titletype.png}
        % \caption{Accuracy - Neural Network}
        \label{fig:accuracy_nn_titletype}
    \end{subfigure}
    \caption{Training and validation loss and accuracy on the \texttt{titleType} classification task.}
    \label{fig:nn_performance_titletype}
\end{figure}

% conf matrix and roc titletype
Table~\ref{tab:nn_report_titletype} reports the performances of
the network on the test set. While Accuracy and Macro-Averaged
F1-Score are comparable to ensemble methods, Macro-averaged
Precision is lower, and Macro-Averaged Recall is higher.
This means that the neural network tends to prioritize recall
over precision, producing more predictions for minority classes
(higher true positive rate) at the expense of increased false
positives (lower precision).

\begin{table}[H]
    \centering
    \begin{tabular}{lccc}
    \hline
     & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score}\\
    \hline
    \textbf{Macro avg}   & 0.71 & 0.82 & 0.73 \\
    \textbf{Weighted avg}& 0.93 & 0.90 & 0.91 \\
    \hline
    \textbf{Accuracy}    & & & 0.90 \\
    \hline
    \end{tabular}
    \caption{Classification performances for the neural network on the \texttt{titleType} classification task.}
    \label{tab:nn_report_titletype}
\end{table}


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/cm_nn_titletype.png}
        \caption{Confusion Matrix - Neural Network}
        \label{fig:cm_nn_titletype}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.50\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/roc_nn_titletype.png}
        \caption{ROC Curve - Neural Network}
        \label{fig:roc_nn_titletype}
    \end{subfigure}
    \caption{Confusion Matrix and ROC Curve for the Neural Network on the \texttt{titleType} classification task.}
    \label{fig:cm_roc_nn_titletype}
\end{figure}

Figure~\ref{fig:cm_nn_rating} shows the confusion matrix. Consistently
good performances are found across the most represented classes, in line
with the previous models.
Figure~\ref{fig:roc_nn_rating} shows the ROC curve, which again
have good performances and are in line with the previous models.

\subsubsection{averageRating task}
This first neural network initially has four branches:
\begin{itemize}
    \item \texttt{titleType}: handles the titleType feature, through
    an Embedding layer, followed by a Flatten layer and
    a Dense layer with 8 units and swish (\textbf{Sigmoid Linear Unit})
    activation;
    \item \texttt{region}: handles the region feature, through
    a Dense layer with 8 units and swish activation;
    \item \texttt{genre}: handles the genre feature, through
    a Dense layer with 8 units and swish activation;
    \item \texttt{numerical}: handles the other numerical
    features, through a Dense layer with 96 units
    and swish activation.
\end{itemize}
The outputs of the four branches are then concatenated and
fed into five Dense layers with 256, 128, 64, 64 and 32 units and
swish activation.
The output layer has 6 units (one for each class) and
softmax activation.
For each Dense layer, Batch Normalization is applied,
in order to stabilize and speed up training.
A dropout of 0.2 is also applied everywhere but in the input
branches, to reduce overfitting. More aggressive dropout rates
were tested, but generally lead to worse performances.

The optimizer's setup and training procedure are the same as for
the \texttt{titleType} task reported in the previous section,
except for the optimizer's learning rate, which was set to
0.0006.\\
% The optimizer used was Adam with a learning rate of 0.0006,
% and the loss function was Categorical Crossentropy.
% The model was trained for a maximum of 500 epochs, with early
% stopping based on validation loss with a patience of 20 epochs.
% The batch size used was 64, and balanced class weights were used to
% address class imbalance.\\


Figure~\ref{fig:nn_performance_rating}
shows the training and validation loss and accuracy over epochs.
While both graphs show a steady improvement in loss and accuracy
over the epochs for training data, the validation curves
tend to flatten out after some epochs, with a bit of instability
in both metrics.
That being said, for the number of epochs before fulfilling
the early stopping condition, overfitting is not an issue for
performances, and although slight, there is still improvement
until stopping.\\


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/loss_rating.png}
        % \caption{Loss - Neural Network}
        \label{fig:loss_nn_rating}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/accuracy_rating.png}
        % \caption{Accuracy - Neural Network}
        \label{fig:accuracy_nn_rating}
    \end{subfigure}
    \caption{Training and validation loss and accuracy on the \texttt{averageRating} classification task.}
    \label{fig:nn_performance_rating}
\end{figure}

Table~\ref{tab:nn_report_rating} summarizes the classification
report for the neural network.
Performances are limited, but this configuration was found to
be the best among those tested, sinc eother architectures and
parameter settings led to either worse results or they ignored
minority classes, as shown in the confusion matrix in figure
\ref{fig:cm_nn_rating}. With respect to the other reported models,
the neural network particularly underperforms on the \texttt{[7, 8)}
class, but spreads predictions more evenly across other classes.

Another important observation is that for each class, the highest
misclassified classes are adjacent ones,
which is expected given the ordinal nature of the target variable.

Figure~\ref{fig:roc_nn_rating} shows the ROC curve for the neural
network. All classes are well above the random classifier line,
with AUCs ranging from 0.67 to 0.84, indicating that the model
still represents a good improvement over random guessing.
The best AUCs are achieved by the most underrepresented classes
(\texttt{[1, 5)} and \texttt{[9, 10)}), suggesting that
the model is better at identifying extreme ratings, while
intermediate ones are harder to distinguish.

\begin{table}[H]
    \centering
    \begin{tabular}{lccc}
    \hline
     & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score}\\
    \hline
    \textbf{Macro avg}   & 0.34 & 0.40 & 0.34 \\
    \textbf{Weighted avg}& 0.40 & 0.35 & 0.36 \\
    \hline
    \textbf{Accuracy}    & & & 0.35 \\
    \hline
    \end{tabular}
    \caption{Classification performances for the neural network on the \texttt{averageRating} classification task.}
    \label{tab:nn_report_rating}
\end{table}


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/cm_nn_rating.png}
        \caption{Confusion Matrix - Neural Network}
        \label{fig:cm_nn_rating}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.53\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/roc_nn_rating.png}
        \caption{ROC Curve - Neural Network}
        \label{fig:roc_nn_rating}
    \end{subfigure}
    \caption{Confusion Matrix and ROC Curve for the Neural Network on the \texttt{averageRating} classification task.}
    \label{fig:cm_roc_nn}
\end{figure}




\subsection{Gradient Boosting Machines}

\subsubsection{titleType task}

% \begin{wraptable}{r}{0.45\textwidth}
% \centering
% \begin{tabular}{lcc}
% \hline
% \textbf{Hyperparameter} & \textbf{titleType} & \textbf{averageRating} \\
% \hline
% \texttt{subsample}            & 0.8   & 0.6   \\
% \texttt{n\_estimators}        & 500   & 400   \\
% \texttt{max\_depth}           & 6     & 5     \\
% \texttt{min\_samples\_split}  & 10    & 10    \\
% \texttt{min\_samples\_leaf}   & 1     & 2     \\
% \texttt{max\_features}        & None  & sqrt  \\
% \texttt{learning\_rate}       & 0.1   & 0.05  \\
% \hline
% \end{tabular}
% \caption{Hyperparameter search space for GBM.}
% \label{tab:GBM_param}
% \end{wraptable}

\begin{wraptable}{r}{0.45\textwidth}
\centering
\begin{tabular}{lc}
\hline
\textbf{Hyperparameter} & \textbf{titleType} \\
\hline
\texttt{subsample}            & 0.8   \\
\texttt{n\_estimators}        & 500   \\
\texttt{max\_depth}           & 6     \\
\texttt{min\_samples\_split}  & 10    \\
\texttt{min\_samples\_leaf}   & 1     \\
\texttt{max\_features}        & None  \\
\texttt{learning\_rate}       & 0.1   \\
\hline
\end{tabular}
\caption{Hyperparameters for \texttt{titleType}.}
\label{tab:GBM_param_titletype}
\end{wraptable}


Next, we applied a \texttt{Gradient Boosting Machine (GBM)} to evaluate its ability to classify the \texttt{titleType} variable. 


Hyperparameter tuning was performed using
\newline\texttt{RandomizedSearchCV} with 5-fold stratified cross-validation.
Table \ref{tab:GBM_param_titletype} shows the best parameters found for the \texttt{titleType} task.


From the table \ref{tab:GBM_report_t}, we can see that GBM performs very well on the \texttt{titleType} task, achieving an accuracy of 0.96.
The confusion matrix (Figure~\ref{fig:GBM_confusion_title}) shows that the model is able to correctly classify most of the classes, 
with only a few misclassifications recorded for the less represented classes \texttt{video} and \texttt{tvSpecial}.
The ROC curves (Figure~\ref{fig:GBM_ROC_title}) also show high AUC values for all classes, 
indicating that the model is able to separate the classes well.
The differences in ROC curve suggest that the model is more effective at distinguishing structured and well-documented 
categories such as \textit{tvEpisode}, \textit{tvSeries}, and \textit{movie}, 
To further investigate, we examined feature importance and found that \texttt{runtimeMinutes}, \texttt{numRegions}, 
and \texttt{directorCredits} were among the most influential features. 
Titles with longer runtimes and broader regional distribution were more likely to be classified as \textit{movies} or \textit{tvSeries}, 
while shorter or regionally limited titles tended to fall into categories like \textit{short} or \textit{video}. 
The number of director credits also might represent a proxy for
production quality and documentation.
Interestingly, the \textit{Europe} feature emerged as a strong predictor, 
possibly due to cultural or market-specific trends in title type distribution. 


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth} 
    \centering
    \includegraphics[width=\textwidth]{plotsss/GBM_confusion_title.png} 
    \caption{Confusion Matrix - Gradient Boosting Machine}
    \label{fig:GBM_confusion_title} 
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth} 
    \centering
    \includegraphics[width=\textwidth]{plotsss/GBM_ROC_title.png} 
    \caption{ROC Curve - Gradient Boosting Machine}
    \label{fig:GBM_ROC_title} 
    \end{subfigure}
    \caption{Confusion Matrix and ROC Curve for the Gradient Boosting Machine on the \texttt{titleType} classification task.}
    \label{fig:cm_roc_gbm}
\end{figure}

% \begin{table}[H]
% \centering
% \begin{minipage}{0.48\textwidth}
%     \centering
%     \begin{tabular}{lccc}
%     \hline
%     \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} \\
%     \hline
%     tvEpisode  & 0.94 & 0.97 & 0.96 \\
%     movie      & 0.95 & 0.98 & 0.97 \\
%     short      & 0.98 & 0.98 & 0.98 \\
%     tvSeries   & 0.94 & 0.97 & 0.96 \\
%     video      & 0.77 & 0.45 & 0.57 \\
%     tvSpecial  & 0.79 & 0.55 & 0.65 \\
%     \hline
%     \textbf{Accuracy}       & \multicolumn{3}{c}{0.96} \\
    % \textbf{Macro avg}      & 0.90 & 0.82 & 0.85 \\
    % \textbf{Weighted avg}   & 0.96 & 0.96 & 0.96 \\
%     \hline
%     \end{tabular}
%     \caption{Classification report for \texttt{titleType}}
%     \label{tab:GBM_report_t}
% \end{minipage}
% \hspace{0.02\textwidth}
% \begin{minipage}{0.48\textwidth}
%     \centering
%     \begin{tabular}{lccc}
%     \hline
%     \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} \\
%     \hline
%     \texttt{[1,5)}   & 0.49 & 0.38 & 0.43 \\
%     \texttt{[5,6)}   & 0.37 & 0.26 & 0.31 \\
%     \texttt{[6,7)}   & 0.41 & 0.39 & 0.40 \\
%     \texttt{[7,8)}   & 0.47 & 0.67 & 0.56 \\
%     \texttt{[8,9)}   & 0.43 & 0.30 & 0.35 \\
%     \texttt{[9,10]}  & 0.44 & 0.22 & 0.29 \\
%     \hline
%     \textbf{Accuracy}       & \multicolumn{3}{c}{0.44} \\
    % \textbf{Macro avg}      & 0.44 & 0.37 & 0.39 \\
    % \textbf{Weighted avg}   & 0.44 & 0.44 & 0.43 \\
%     \hline
%     \end{tabular}
%     \caption{Classification report for \texttt{averageRating}}
%     \label{tab:GBM_report_r}
% \end{minipage}
% \end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{lccc}
    \hline
     & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score}\\
    \hline
    \textbf{Macro avg}      & 0.90 & 0.82 & 0.85 \\
    \textbf{Weighted avg}   & 0.96 & 0.96 & 0.96 \\
    \hline
    \textbf{Accuracy}    & & & 0.96 \\
    \hline
    \end{tabular}
    \caption{Classification performances for the GBM on the \texttt{titleType} classification task.}
    \label{tab:GBM_report_t}
\end{table}









\subsubsection{averageRating task}



\begin{wraptable}{r}{0.45\textwidth}
\centering
\begin{tabular}{lc}
\hline
\textbf{Hyperparameter} & \textbf{averageRating} \\
\hline
\texttt{subsample}            & 0.6   \\
\texttt{n\_estimators}        & 400   \\
\texttt{max\_depth}           & 5     \\
\texttt{min\_samples\_split}  & 10    \\
\texttt{min\_samples\_leaf}   & 2     \\
\texttt{max\_features}        & sqrt  \\
\texttt{learning\_rate}       & 0.05  \\
\hline
\end{tabular}
\caption{Hyperparameters for \texttt{averageRating}.}
\label{tab:GBM_param_rating}
\end{wraptable}

Hyperparameter tuning was performed using
\newline\texttt{RandomizedSearchCV} with 5-fold stratified cross-validation.
Table \ref{tab:GBM_param_rating} shows the best parameters found for the \texttt{averageRating} task.


From the table \ref{tab:GBM_report_r}, it is evident that the Gradient Boosting Machine performs moderately on the 
\texttt{averageRating} classification task, achieving an overall accuracy of 0.44.
Examination of the confusion matrix
(Figure~\ref{fig:GBM_confusion_rating}) reveals that the model correctly
classifies rating class [6,7) more reliably than others.
A noteworthy observation is that misclassifications tend to occur
between neighboring classes, whereas classes that are further apart
are rarely confused. 
This suggests that the proximity and overlap of class distributions
presents inherent challenges in distinguishing closely spaced ratings.

The ROC curves (Figure~\ref{fig:GBM_ROC_rating}) are reported in
figure~\ref{fig:GBM_ROC_rating}.
Feature importance analysis provides additional insight:
\textit{ratingCount} emerges as the strongest predictor. 
\textit{StartYear} appears influential, potentially reflecting temporal
trends in ratings, while \textit{runtimeMinutes} indicates that longer
titles tend to receive higher ratings. 
% Other features such as \textit{totalCredits}, \textit{castNumber},
% \textit{totalMedia}, and \textit{tvEpisode} also contribute significantly, 
% with \textit{tvEpisode} being more impactful than other TV formats like \textit{tvMiniseries}, \textit{tvSpecial}, or \textit{tvShort}, 
% possibly due to greater representation in the dataset.
Overall, the results suggest that while the model captures broad
patterns, it struggles with fine-grained distinctions between closely
spaced rating bins. 


\begin{table}[H]
    \centering
    \begin{tabular}{lccc}
    \hline
     & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score}\\
    \hline
    \textbf{Macro avg}      & 0.44 & 0.37 & 0.39 \\
    \textbf{Weighted avg}   & 0.44 & 0.44 & 0.43 \\
    \hline
    \textbf{Accuracy}    & & & 0.44 \\
    \hline
    \end{tabular}
    \caption{Classification performances for the GBM on the \texttt{titleType} classification task.}
    \label{tab:GBM_report_r}
\end{table}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth} 
    \centering
    \includegraphics[width=\textwidth]{plotsss/GBM_confusion_rating.png} 
    \caption{Confusion Matrix - Gradient Boosting Machine}
    \label{fig:GBM_confusion_rating} 
    \end{subfigure}
    \begin{subfigure}[b]{0.485\textwidth} 
    \centering
    \includegraphics[width=\textwidth]{plotsss/GBM_ROC_rating.png} 
    \caption{ROC Curve - Gradient Boosting Machine}
    \label{fig:GBM_ROC_rating} 
    \end{subfigure}
    \caption{Confusion Matrix and ROC Curve for the Gradient Boosting Machine on the \texttt{averageRating} classification task.}
    \label{fig:cm_roc_GBM_rating}
\end{figure}



\subsection{Model Comparison}

In this subsection, we compared the performance of all advanced
classification models applied to the two target variables,
\texttt{titleType} and \texttt{averageRating}.
To support this analysis, we included a summary table reporting the
performance metrics of all models in
Table~\ref{tab:merged_model_comparison}.

\begin{table}[h!]
\centering
\begin{tabular}{lcccc}
\hline
\multirow{2}{*}{\textbf{Model}} 
& \multicolumn{2}{c}{\textbf{titleType}} 
& \multicolumn{2}{c}{\textbf{averageRating}} \\
\cline{2-5}
& \textbf{Accuracy} & \textbf{F1-Macro} 
& \textbf{Accuracy} & \textbf{F1-Macro} \\
\hline
Logistic Regression        & 0.75 & 0.54 & 0.27 & 0.25 \\
SVM (RBF)                  & 0.90 & 0.64 & 0.39 & 0.28 \\
Random Forest              & 0.92 & 0.75 & 0.45 & 0.37 \\
AdaBoost                   & 0.92 & 0.73 & 0.43 & 0.35 \\
Neural Network             & 0.90 & 0.73 & 0.35 & 0.34 \\
Gradient Boosting Machine  & 0.96 & 0.85 & 0.44 & 0.39 \\
\hline
\end{tabular}
\caption{Comparison of model performance for the \texttt{titleType} and \texttt{averageRating} classification tasks.}
\label{tab:merged_model_comparison}
\end{table}

Across both tasks, ensemble-based methods such as Gradient Boosting
Machines and Random Forests consistently outperformed linear models,
confirming that nonlinear relationships played a significant role in
the IMDb dataset. Logistic Regression, despite its simplicity,
provided a strong baseline for the \texttt{titleType} 
task but struggled with the \texttt{averageRating} classes.
Support Vector Machines achieved competitive performance, 
particularly with the RBF kernel, but required substantial
computational resources.
Neural Networks demonstrated good predictive power, but were
outperformed in both tasks by other models.

Finally, the best performer on both was Gradient Boosting Machine,
particularly on the \texttt{titleType} task. Random Forest had very
comparable results on the \texttt{averageRating} task, with slightly
higher accuracy and lower Macro-Averaged F1-Score.



\subsection{Explainable AI}

To better understand the predictions made by our Neural Network model,
we employed two model-agnostic explanation techniques:
\texttt{LIME} (Local Interpretable Model-agnostic Explanations) and
\texttt{SHAP} (SHapley Additive exPlanations).
For our analysis, we selected the first misclassified instance from the
test set of both classification tasks, \texttt{titleType} and
\texttt{averageRating}, and generated explanations with both methods,
displaying only the top five features for clarity.


\subsubsection{LIME}

\paragraph*{titleType}

For the \texttt{titleType} neural network, we analyzed a misclassified instance 
where the true class was \texttt{tvEpisode}, but the model predicted \texttt{tvSeries} with very high confidence. 
The \texttt{LIME} visualization in Figure~\ref{fig:LIME_title} shows
how the top 5 input features contributed to the final prediction.
% Each horizontal bar represents a simple rule on one feature (for
% example, \texttt{totalCredits} exceeding a certain threshold or
% \texttt{numRegions} being above a given value), 
% and the associated number indicates how strongly that rule pushes the
% local surrogate model towards or away from a specific class.

Positive contributions increase the probability of the class being explained (in this case \texttt{tvSeries}), 
while negative contributions reduce it. The cumulative effect of all these contributions, starting from a baseline, 
leads to the final predicted probability displayed at the top-left.
The labels such as \texttt{tvSeries} and \texttt{NOT tvSeries} (and analogously for other classes) reflect the one-vs-rest formulation 
used by \texttt{LIME} for each class.
\texttt{LIME} fits a simple linear surrogate model that approximates
the Neural Network's decision boundary in the neighborhood of the
instance.
% Bars aligned with \texttt{tvSeries} indicate conditions that make
% this instance look more like a typical \texttt{tvSeries}, 
% whereas bars aligned with \texttt{NOT tvSeries} capture conditions
% that would instead support other classes. 

The same feature can appear with different signs across different
class panels, meaning that it pushes the prediction towards one class
while simultaneously pulling it away from another.
The numerical values attached to each rule can therefore be 
interpreted as local importance scores: larger magnitudes (in absolute
value) correspond to a stronger influence on the final decision. 
% Overall, the figure conveys that specific combinations 
% of metadata, such as high \texttt{totalCredits} or a large number of regions, locally made the 
% model treat this \texttt{tvEpisode} more like a \texttt{tvSeries}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{plotsss/Lime_title.png}
    \caption{LIME explanation for the \texttt{titleType} prediction of first misclassified instance in the test set.}
    \label{fig:LIME_title}
\end{figure}


\paragraph*{averageRating}


For the \texttt{averageRating} task, the behaviour of the model is
qualitatively different from the \texttt{titleType} example.
Here, the true class of the instance is \([6, 7)\), whereas the neural
network predicts the lowest class \([1, 5)\) with the highest
probability. 
% for each rating bin, a set of feature-based rules (e.g., conditions on \texttt{ratingCount}, 
% \texttt{runtimeMinutes}, or other metadata) together with their local contributions, 
% indicating how strongly they push the surrogate model towards or away from that bin. 
Unlike the title type case, where the error involved two semantically
similar categories, this misclassification is not from a class to
a neighboring one.
% This makes the explanation particularly interesting, 
% and the explanation helps to reveal why 
% the model locally treats this title as if it belonged to the lowest-rated group.
Figure~\ref{fig:LIME_rating} shows, \texttt{castNumber} and \texttt{totalNominations}
have positive weights.
% For example, relatively 
% low \texttt{ratingCount} or specific combinations of content-related features 
% may be interpreted by the model as indicators of poor reception, thereby increasing 
% the probability assigned to \([1, 5)\)
 
In contrast, when looking at the panel for the 
true class \([6, 7)\), several of these same features tend to appear with negative contributions, 
indicating that, they move the prediction \emph{away} from 
\([6, 7)\) and towards competing classes. This sign inversion captures the fact that the 
instance does not match well the typical profile learned by the network for mid-range ratings, 
even though it is labelled as such in the ground truth. The negative values therefore 
do not imply that the features are globally bad predictors, but that, for this particular 
title, they locally push against assigning it to \([6, 7)\).


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{plotsss/Lime_rating.png}
    \caption{LIME explanation for the \texttt{averageRating} prediction of first misclassified instance}
    \label{fig:LIME_rating}
\end{figure}


\newpage
\subsubsection{SHAP}


\paragraph*{titleType} 

\begin{wrapfigure}{r}{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{plotsss/SHAP_title.png}
    \caption{SHAP explanation for \texttt{titleType}}
    \label{fig:SHAP_title}
\end{wrapfigure}


We applied \texttt{SHAP} to the same misclassified instance where the 
true class was \texttt{tvEpisode} and the model predicted
\texttt{tvSeries}. 
The model's output for this class was $f(x) = 0.367$, slightly above
the expected value $E[f(X)] = 0.29$, indicating a modest increase in
confidence 
driven by the instance's feature profile. SHAP decomposes this shift into 
additive contributions from individual features. The most influential was 
\texttt{genre1}, with a SHAP value of $-0.24$, acting as a strong 
counterweight against the prediction. 
In contrast, \texttt{genre2} ($+0.18$), \texttt{Europe} ($+0.11$), and
\texttt{Oceania} ($+0.07$) contributed positively.
% suggesting a non-linear interaction captured by the model. 
% \texttt{South America} contributed $-0.06$, further reducing the score. 
The remaining 18 features collectively added only $+0.01$, reinforcing that 
the prediction was shaped by a small set of dominant signals.\\





\paragraph*{averageRating}


\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{plotsss/SHAP_rating.png}
    \caption{SHAP explanation for \texttt{averageRating}}
    \label{fig:SHAP_rating}
\end{wrapfigure}

% \centering
%     \includegraphics[width=0.5\linewidth]{plotsss/SHAP_rating.png}
%     \caption{SHAP explanation for the \texttt{averageRating} prediction of the first misclassified instance in the test set.}
%     \label{fig:SHAP_rating}


For the same misclassified instance in the
\newline\texttt{averageRating} task
(\texttt{true}: \([6, 7)\), \texttt{predicted}: \([1, 5)\)), 
the SHAP explanation in Figure~\ref{fig:SHAP_rating} reveals 
the model's output increased slightly from the expected value
($E[f(X)] = 0.126$ to $f(x) = 0.135$). 
The most influential feature was \texttt{startYear} ($+0.08$),
which pushed
the prediction toward the predicted class, while \texttt{numRegions}
($-0.03$), \texttt{ratingCount} ($-0.02$), and \texttt{totalCredits}
($-0.02$) contributed negatively on the prediction. \\

\subsubsection{Explainable AI Conclusion}

% Taken together, for the same misclassified instance the contrast 
% in feature attribution is striking.
For \texttt{titleType}, 
\texttt{LIME} emphasized features 
such as \texttt{totalCredits} and \texttt{numRegions}, interpreting
them as locally decisive for the misclassification.
SHAP, however, highlighted genre 
and geographic indicators as the primary drivers.
The fact that \texttt{genre1} had the largest 
negative SHAP value ($-0.24$) yet was not prominent in LIME's
explanation.

For \texttt{averageRating} task the top features identified by \texttt{LIME} were \texttt{castNumber}, \texttt{totalNominations}, and
\newline\texttt{tvEpisode}, 
all contributing positively to the prediction of the lowest rating bin.
In contrast, \texttt{SHAP} attributed the prediction 
primarily to \texttt{startYear} ($+0.08$), followed by negative contributions from \texttt{numRegions} ($-0.03$), \texttt{ratingCount} 
($-0.02$), and \texttt{totalCredits} ($-0.02$). While both methods
assigned a high relevance to \texttt{ratingCount}, 
the presence of \texttt{startYear} in SHAP and its absence 
in LIME suggests that temporal context was not captured by the
latter's linear local surrogate.