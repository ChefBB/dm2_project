\section{Outliers}
\label{sec:outlier}

This section investigates the presence and impact of outliers in the
dataset using multiple unsupervised anomaly detection techniques.
The goal is to assess how outlier removal affects downstream
classification performance and to compare the behavior of different
detection methods.


\subsection{Finding Baseline}
In this section, our primary objective was to identify the top outliers in the dataset using three methods
(\texttt{Local Outlier Factor, Isolation Forest}, and \texttt{Angle-Based Outlier Detection}). 
To establish a baseline for model performance, we initially employed two supervised learning algorithms
(\texttt{Decision Tree} and \texttt{kNN}).
To enhance the alignment between the later module tasks we defined a categorical target variable, \texttt{rating\_bin}, 
by dividing the continuous \texttt{averageRating} into six distinct classes. 
The classes were defined as follows:

    $$0 \leftarrow [1-5)\text{; }1 \leftarrow [5-6)\text{; }2 \leftarrow [6-7)\text{; }
    3 \leftarrow [7-8)\text{; } 4 \leftarrow [8-9)\text{; } 5 \leftarrow [9-10]$$

The value count for each class was 12856, 19576, 37032, 49164, 25414, and 5489 respectively.\\
We then performed a grid search with cross-validation \texttt{(GridSearchCV)} on both models to identify the best hyperparameters. 
Since our dataset was large, we conducted the grid search on a 10\% stratified sample to optimize computational efficiency. 
For K-NN, the optimal parameters were:
\begin{center}
    \texttt{metric = manhattan, n\_neighbors = 50, weights = distance}
\end{center} 

For Decision Tree, the optimal parameters were:
\begin{center}
    \texttt{criterion = gini, max\_depth = 10, min\_samples\_leaf = 4, min\_samples\_split = 10}
\end{center}
We trained both models on the entire training dataset and evaluated their performance on the test set. 
We split our training dataset into training (80\%) and validation (20\%) sets. We computed the baseline accuracy 
without removing any outliers, which was 36\% for \texttt{K-NN} and 32\% for \texttt{Decision Tree}.

\newpage
\subsection{Finding Threshold}

\begin{wraptable}{r}{0.4\textwidth}
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
 &  & \textbf{LOF} & \textbf{IF} & \textbf{ABOD} \\ \hline
\multirow{2}{*}{\textbf{1\%}} & \textbf{KNN} & 42 & 42 & 41 \\ \cline{2-5} 
 & \textbf{DT} & 39 & 39 & 40 \\ \hline
\multirow{2}{*}{\textbf{5\%}} & \textbf{KNN} & 42 & 42 & 41 \\ \cline{2-5} 
 & \textbf{DT} & 39 & 39 & 40 \\ \hline
\multirow{2}{*}{\textbf{10\%}} & \textbf{KNN} & \textbf{43} & 42 & 41 \\ \cline{2-5} 
 & \textbf{DT} & \textbf{40} & 39 & 40 \\ \hline
\end{tabular}
\caption{Accuracy at different threshold}
\label{tab: threshold}
\end{wraptable}
Next, we applied the three outlier detection methods to identify and remove outliers from the training dataset.
We experimented with different contamination levels \texttt{(0.01, 0.05, 0.1)} to 
determine the optimal proportion of outliers to remove.
After removing the identified outliers, we retrained both models on the cleaned training dataset and evaluated their performance on the test set.
We observed that removing outliers generally improved model accuracy, but different contamination levels had no significant impact on the results as shown in table \ref{tab: threshold}.
Only the results from \texttt{LOF} with a contamination level of 0.1 showed an improvement of 1\%. 
Therefore, we fixed the contamination level at 10\% for all three methods to maintain consistency. 
% Subsequently, we combined the results of the three methods by aggregating the outlier score and 
% removed those rows that were flagged as outliers by at least two out of the three methods, 
% which accounted for approximately 3\% of the dataset.


\subsection{Outlier Detection Conclusion}
In conclusion, our anomaly detection task revealed a consistent trend: both models \texttt{KNN} and \texttt{DT} performed better than the 
baseline after the removal of outliers. This outcome sets the foundation for our subsequent focus on model 
explainability in the later sections.

From the T-SNE visualization, we observed that \texttt{LOF} predominantly 
identified outliers at the edges of the data distribution \ref{fig:LOF}, whereas \texttt{IF} detected 
them primarily within a clustered region \ref{fig:ISF}. \texttt{ABOD}, on the other hand, captured outliers both at 
the edges and within dense clusters \ref{fig:ABOD}. This variation can be explained by the fundamental 
differences between the algorithms: \texttt{LOF} emphasizes anomalies in low-density regions, 
while \texttt{IF} isolates points distant from the main distribution. 
\texttt{ABOD}, which evaluates outliers based on the angular relationships of points, reflects a hybrid behavior by 
marking anomalies in both sparse and dense regions. Importantly, since the dataset is dominated by a 
single high-density region, most points lack anomalous neighbors, explaining why \texttt{ABOD} highlighted relatively 
fewer strong anomalies. To ensure computational efficiency, we implemented \texttt{ABOD}'s fast version.

Overall, as we can see from table \ref{tab: threshold} the removal of outliers with different threshold 
led to only marginal gains in predictive performance.
However, the differences in detection patterns across methods provide valuable insights into the structure and density 
of the data distribution, reinforcing the importance of combining multiple approaches for a more 
comprehensive anomaly detection strategy.
% Therefore, we combined the results of all three methods to identify the common outliers.
Therefore, we combined the results of the three methods by aggregating the outlier score and 
removed those rows that were flagged as outliers by at least two out of the three methods, 
which accounted for approximately 3\% of the dataset.


\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/LOF.png}
        \caption{Local Outlier Factor}
        \label{fig:LOF}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/ISF.png}
        \caption{Isolation Forest}
        \label{fig:ISF}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/ABOD.png}
        \caption{Angle-Based Outlier Detection}
        \label{fig:ABOD}
    \end{subfigure}
    \caption{Comparison of T-SNE visualizations for different outlier detection methods.}
    \label{fig:outlier_methods}
\end{figure}

% Figure~\ref{fig:tsne_outliers} presents a comparative T-SNE visualization of the outlier scores 
% produced by Local Outlier Factor (LOF), Isolation Forest (IF), and Angle-Based Outlier Detection (ABOD). 
% Each method reveals distinct spatial patterns in how anomalies are distributed across the feature space. 
% LOF (Figure~\ref{fig:tsne_outliers}a) primarily flagged points at the periphery of the data cloud, 
% consistent with its sensitivity to local density variations. IF (Figure~\ref{fig:tsne_outliers}b) 
% identified outliers within compact clusters, suggesting that it isolates points based on their 
% distance from the overall distribution rather than local sparsity. ABOD (Figure~\ref{fig:tsne_outliers}c) 
% exhibited a hybrid behaviour, marking anomalies both at the edges and within dense regions, 
% reflecting its angular-based scoring mechanism. These visual differences confirm the complementary 
% nature of the three algorithms. LOF excels at detecting boundary anomalies, IF captures global isolation, 
% and ABOD balances both perspectives. Importantly, the dataset is dominated by a single high-density region, 
% which explains why ABOD flagged fewer strong anomalies—most points lacked sufficient angular separation 
% from their neighbours. To ensure computational efficiency, we used the fast approximation of ABOD with a 
% fixed number of neighbors. Despite these algorithmic differences, Table~\ref{tab:outlier_accuracy} shows 
% that removing outliers—regardless of method or contamination level—led to only marginal improvements in 
% classification accuracy (1–2\% gain). This suggests that while outlier removal helps stabilize model 
% performance, it does not drastically alter predictive outcomes. However, the consistency of improvement 
% across both KNN and Decision Tree models validates the utility of anomaly filtering as a preprocessing step. 
% To balance precision and coverage, we aggregated the outlier scores and removed instances flagged by at 
% least two of the three methods, resulting in a final exclusion of approximately 3\% of the dataset.