\section{Time Series Analysis}

The following chapter illustrates Time Series analysis.
This was done on a separate dataset, consisting of 1134 time series.
Each of them represented daily domestic box-office gross revenues in
the United States and Canada, spanning 100 days from release day (day
0) to day 99.
% The following analysis revolve around both time series-specific
% and more general techniques.
Each observation also includes descriptive metadata, adding
additional informations on titles.
% : \texttt{id},
% \texttt{genre} and \texttt{rating}.

\subsection{Data Understanding}

The dataset contains 104 attributes in  total: 100 numerical columns
corresponding to daily gross revenues, one numerical column for the
IMDb average \texttt{rating}, and three categorical columns
identifying the film (\texttt{id}, \texttt{genre}, and
\texttt{rating category}).
Preliminary inspection revealed no missing values.


% TODO: check
Descriptive statistics provide an overview of the box-office revenue trends. 
On release day, the average revenue is approximately 9 million USD,
with maximum values exceeding 150 million USD. 
Revenues decline rapidly in subsequent days, reaching mean values near
100000 USD by day 99.
Variance remains high across the series, indicating substantial
variability in revenue levels among films.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.65\textwidth]{plotsss/ts_distrib.png}
%     \caption{Log-scale box-office gross revenues}
%     \label{fig:ts_distrib}
% \end{figure}

\begin{figure}[ht]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plotsss/ts_boxplot.png} 
        \captionof{figure}{Boxplot of box-office gross revenues}
        \label{fig:ts_boxplot} 
    \end{minipage}
    \hfill
    \begin{minipage}{0.5\textwidth} 
        \includegraphics[width=\textwidth]{plotsss/ts_distrib.png}
        \caption{Log-scale box-office gross revenues}
        \label{fig:ts_distrib}
    \end{minipage}

\end{figure}

Figure~\ref{fig:ts_boxplot} shows the boxplot distribution of all time series.
We can see a lot of fliers, especially in the first days after release,
indicating high variability in box-office revenues among different films.
All fliers are concentrated in the upper range, showing that data is
right-skewed, with a few films achieving exceptionally high revenues
compared to the majority. 

Figure~\ref{fig:ts_distrib} illustrates the log-scaled distribution of
all time series, grouped by rating class.
For each class, the mean time series is reported together with the
corresponding minimum and maximum values at each time step.
Logarithmic scale was adopted to better capture the strong seasonal
patterns present in the data, which are otherwise compressed in the
later time steps when using a linear scale.
From this plot, a downward trend is visible in all classes, showing
higher engagement at release.

Another important note on the dataset is that missing values for films with 
runs shorter than 100 time steps were completed through a synthetic extension
procedure.\\

Although a 7-step seasonal pattern appears to be present, the seasonality is 
much more pronounced at a 14-step interval. This suggests that the time series may 
consist of two observations per day, making a 14-step cycle correspond to a 
weekly pattern.
% This could be due to the fact that data were synthesized in between existing 
% observations.
This could indicate that the synthetic extensions may have been interleaved within 
the original time steps rather than appended only at the tail of the series. 
The insertion of noise-augmented mean values within the temporal sequence 
might alter the effective sampling structure and artificially reinforce 
longer-period seasonal patterns.
This interpretation seems to be consistent with the box office domain, where 
revenue peaks are typically observed during weekends, particularly around a 
movieâ€™s release period. Moreover, in this context, 7 steps peaks would be
explained by midweek promotional activities.
\paragraph*{{Metadata}}
IMDb ratings show a mean of 6.6 with a standard deviation of 0.9.
The distribution ranged from 2.8 to 8.7.
The \texttt{rating\_category} variable is a binning of \texttt{rating},
useful for classification tasks in later sections.
The feature consists of five different classes:
\begin{itemize}
    % Low (10 titles), Medium Low (128), Medium (387), Medium High (232), and High (377). 
    \item \textbf{Low} (10 titles), representing ratings ranging from
    1.0 to 4.0;
    \item \textbf{Medium Low} (128 titles), representing ratings ranging from 4.1 to 5.5;
    \item \textbf{Medium} (387 titles), representing ratings ranging from 5.6 to 6.5;
    \item \textbf{Medium High} (232 titles), representing ratings from 6.6 to 7.0;
    \item \textbf{High} (377 titles), representing ratings from 7.1 to 10.0.
\end{itemize}
As can be seen, the distribution is highly imbalanced, with the Low
category being significantly underrepresented compared to the other
classes.



% TODO aggiungere parte di scaling eventuale o cose simili di preparation

\input{motifs_discords.tex}



\input{ts_clustering.tex}

\subsection{Classification}

The classification task aims to predict the \texttt{rating\_category} of a film based on its daily box-office revenue time series.
This category originally had five classes: Low, Medium Low, Medium, Medium High, and High.
However, due to the significant class imbalance, with the Low category containing only 10 instances,
the decision was made to merge the Low and Medium Low categories into a single class.

\input{ts_KNN.tex}


\subsubsection{Shapelet-based Classification}
To move beyond global similarity measures and explicitly capture local
discriminative patterns in the revenue dynamics, the classification
task was also addressed using shapelet-based methods, using the 
extracted shapelet features to train a Random Forest classifier.\\

% This approach is particularly well suited to the present task, as
% films with similar overall revenue levels may still differ in
% localized behaviors such as early peaks, decay rates, or delayed
% growth.\\

The preprocessing pipeline first applied a logarithmic transformation
to the raw revenue time series to reduce skewness and stabilize
variance, followed by a per-series mean-variance normalization to
emphasize temporal shape over absolute scale. This choice is motivated
by the nature of shapelet-based methods, which aim to identify
discriminative local patterns whose relevance lies in their temporal
structure rather than in absolute revenue magnitude.

A \texttt{RandomShapeletTransform} was then fitted on the
training data to extract a fixed set of discriminative subsequences,
which were used to transform both training and test sets into feature
representations.
The minimum shapelet length was set to 7, while the maximum was set to
29. Other configurations were also tested, but were discarded because
of worse performances.\\

\begin{wraptable}{r}{0.35\textwidth}
    \centering
    \caption{Random Forest's overview}
    \label{tab:rf_best_params}
    \begin{tabular}{lc}
        \hline
        \textbf{Hyperparameter} & \textbf{Value} \\
        \hline
        \texttt{n\_estimators} & 100 \\
        \texttt{max\_depth} & 20 \\
        \texttt{min\_samples\_split} & 10 \\
        \texttt{min\_samples\_leaf} & 4 \\
        \texttt{max\_features} & \texttt{sqrt} \\
        \hline
        \textbf{Accuracy} & 0.46 \\
        \textbf{Macro F1} & 0.42 \\
        \hline
    \end{tabular}
\end{wraptable}

The Random Forest classifier used balanced class weights and was tuned
via a randomized hyperparameter search.
A \texttt{RandomizedSearchCV} procedure with 20 sampled configurations
and 3-fold cross-validation was employed, optimizing for balanced
accuracy to account for class imbalance.
Table~\ref{tab:rf_best_params} reports the best configuration, along
with key performance metrics.

The selected hyperparameters reflect a balanced model: sufficient trees
and depth for learning, while minimum sample thresholds and feature
subsampling help prevent overfitting.
The model achieves an overall accuracy of 46\%, consistent with the
previous model, while the macro-averaged F1-score is higher, indicating
improved handling of minority classes.

This is confirmed by the confusion matrix in
Figure~\ref{fig:cm_shapelets}.
The \textbf{Low} class is better captured relative to the kNN
classifiers. Errors for this and the \textbf{Medium High} class
are often close to the correct category.
The \textbf{Medium} and \textbf{High} classes are often
misclassified as each other, similar to the kNN models.
The \textbf{High} class performs slightly worse with respect to
the kNN models.

Figure~\ref{fig:shapelets} shows the eight shapelets with the highest
Information Gain. As indicated in the legend, the most informative
shapelets come from the \textbf{Low} and \textbf{Medium High} classes,
which are the least represented.
The top shapelet for the \textbf{High} class ranks eleventh overall,
while the top \textbf{Medium} class shapelet ranks sixteenth.

Notably, the shapelets vary considerably in length, shape, and position
within their respective time series, reflecting the diverse patterns the
model leverages for classification.


\begin{figure}[H]
    \centering
    \begin{minipage}{0.43\textwidth}
        \includegraphics[width=1\textwidth]{plotsss/cm_shapelets.png}
        \caption{Confusion matrix for the RF model based on shapelets}
        \label{fig:cm_shapelets}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=1.\textwidth]{plotsss/shapelets.png}
        \caption{8 most important shapelets}
        \label{fig:shapelets}
    \end{minipage}
\end{figure}


Figures~\ref{fig:toplow},~\ref{fig:topmedium},~\ref{fig:topmediumhigh},
and~\ref{fig:tophigh} show the most informative shapelets for the
Random Forest model, grouped by class.
Each shapelet is compared with the top four SAX motifs of the same
length, allowing a direct comparison of their shapes and illustrating
how discriminative subsequences relate to commonly recurring patterns
in the time series.

The shapelets for the \textbf{Low} and \textbf{High} classes generally
follow the shape of the corresponding motifs.
For the \textbf{Medium} class, the shape aligns with the \texttt{cbbb}
SAX motif, albeit shifted by a few time steps.
In contrast, the \textbf{Medium High} shapelet differs from all motifs
of that length; however, its form is consistent with parts of the extended
motifs identified in Section~\ref{subsec:motifs_discords}, suggesting
it likely still represents a non-anomalous pattern.


\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=1\textwidth]{plotsss/toplow.png}
        \caption{Best shapelet for the Low class}
        \label{fig:toplow}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=1\textwidth]{plotsss/topmedium.png}
        \caption{Best shapelet for the Medium class}
        \label{fig:topmedium}
    \end{minipage}
\end{figure}
    
\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=1\textwidth]{plotsss/topmediumhigh.png}
        \caption{Best shapelet for the Medium High class}
        \label{fig:topmediumhigh}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=1\textwidth]{plotsss/tophigh.png}
        \caption{Best shapelet for the High class}
        \label{fig:tophigh}
    \end{minipage}
\end{figure}


\subsubsection{Recurrent Neural Network}

\begin{wrapfigure}{r}{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{plotsss/rnn_model.png}
    \caption{Architecture of the RNN model}
    \label{fig:rnn_model}
\end{wrapfigure}

As the last model, a Recurrent Neural Network (RNN) was
implemented, because of the suitability of RNNs for
sequential data.
The preprocessing applied was log-scale transformation, followed
by global scaling, in order to preserve absolute scale.

Its architecture is shown in Figure~\ref{fig:rnn_model},
and was obtained through experimentation with
different configurations and hyperparameters.
The test and validation sets used 40\% of the dataset each,
while the training set used the remaining 60\%.
The split was stratified, to maintain class proportions across sets.

The genre features are handled the same way as in previous Neural
Networks (a Dense layer with 8 neurons and ReLU activation function,
the right-most branch in figure~\ref{fig:rnn_model}).

The time series data is processed through an encoder-decoder
architecture, to extract relevant features from the sequences.
The encoder processes the first half of the time series,
with two Bidirectional LSTM layers (with 32 and 64 units respectively).
The decoder processes the second half of the time series,
with two Bidirectional LSTM layers (with 64 and 32 units respectively).
The final shape resembles a diamond shape, with the number of units
first increasing, then decreasing, to capture both local and global
patterns in the data.
Both the encoder and decoder use recurrent dropout of 0.3 in every layer
for regularization.




The outputs of the two branches are finally concatenated, and
fed to a Dense layer with 64 neurons and ReLU activation function,
followed by a Dropout layer with rate 0.3.
The final output layer has 4 neurons (one for each class)
and a Softmax activation function.
Wherever possible, Batch Normalization was applied to speed up training
and improve stability.\\

The model was trained using the Adam optimizer, categorical
cross-entropy loss function and a learning rate of 0.005.
Early stopping was employed to halt training if the validation loss
did not improve for 20 consecutive epochs.
The model was trained for a maximum of 200 epochs with a batch size
of 32, considering balanced class weights.

Figure~\ref{fig:loss_acc} shows the evolution of training and
validation loss and accuracy over epochs.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{plotsss/ts_loss_acc.png}
    \caption{Training and validation loss and accuracy over epochs}
    \label{fig:loss_acc}
\end{figure}

\begin{wrapfigure}{r}{0.4\linewidth}
    \centering
    \includegraphics[width=\linewidth]{plotsss/rnn_cm.png}
    \caption{Confusion matrix for the RNN model on the test set}
    \label{fig:cm_rnn}
\end{wrapfigure}

The loss curves show incremental improvement over epochs,
with relative stability.
Their shape do not indicate overfitting, as the validation loss
seems to find relative stability around the 50th epoch.
Accuracy curves seem to confirm that overfitting is not a central
issue.
They also show significant instability, likely due to the small
size of the dataset, which makes the small sizes of validation
and test sets more susceptible to changes in the model's behavior.

Figure~\ref{fig:cm_rnn} shows the confusion matrix for the RNN
model on the test set.

Both the \textbf{Low} and \textbf{Medium High} classes exhibit
weaker performance, likely due to their relatively small number
of samples. The use of balanced class weights appears to
increase confusion with adjacent classes, as the model assigns
similar importance to neighboring categories.

The \textbf{Medium} class is generally the most challenging to
distinguish.
It also accounts for the majority of non-adjacent
misclassifications, with instances frequently predicted as
\textbf{High}. This behavior suggests a substantial overlap
between the feature distributions of the \textbf{Medium}
class and the surrounding classes.

Table~\ref{tab:macro_weighted_avg} reports the macro- and
weighted-average precision, recall, and F1-score for the
RNN model.
Despite the limited dataset size and class imbalance, the
final model achieves a satisfactory accuracy.
Moreover, the macro and weighted averages indicate reasonably
consistent performance across all classes, without excessive
bias toward the most frequent ones.
    

\begin{table}[H]
    \centering
    \begin{tabular}{lccc}
        \toprule
        \textbf{Metric} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} \\
        \midrule
        \textbf{Macro avg}    & 0.45 & 0.47 & 0.45 \\
        \textbf{Weighted avg} & 0.49 & 0.48 & 0.47 \\
        \midrule
        \textbf{Accuracy}     & & & 0.48 \\
        \bottomrule
    \end{tabular}
    \caption{Macro and weighted average precision, recall, and F1-score for the RNN model}
    \label{tab:macro_weighted_avg}
\end{table} 

\subsubsection{Model Comparison}

Overall, all approaches achieve comparable accuracy values, ranging
between 42\% and 48\%, reflecting the intrinsic difficulty of the
task and the strong overlap between rating classes.

The \textit{k-NN} classifiers provide a simple and interpretable baseline.
The Euclidean-distance variant slightly outperforms the DTW-based
version, suggesting that absolute trends and magnitude-related patterns
are more informative for rating prediction than fine-grained temporal
alignments.
Both variants struggle with minority classes and mainly confuse
adjacent rating levels, consistent with the ordinal nature of the
target.

The shapelet-based Random Forest achieves similar accuracy to the
\textit{k-NN} baseline but improves the macro-averaged F1-score,
indicating a better balance across classes.
This improvement is primarily driven by enhanced recognition of
underrepresented classes, particularly \textbf{Low}, at the cost of a
slight degradation in performance for the \textbf{High} class.

The RNN model attains the highest overall accuracy and competitive
macro- and weighted-average scores.
While its performance gains over classical methods are modest,
the RNN shows more consistent behavior across classes, without being
overly biased toward the majority ones.
However, training instability and residual confusion between
\textbf{Medium} and \textbf{High} classes suggest that model capacity
is constrained by the limited dataset size.
