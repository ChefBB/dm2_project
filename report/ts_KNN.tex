\subsection{kNN Classifier}

%Preprocessing in rnn

The task was first addressed using the $k$-Nearest Neighbors (KNN) classifier with two 
different distance measures, namely Euclidean distance and Dynamic Time Warping (DTW), 
in order to compare a standard vector-based similarity with a time-aware distance specifically 
designed for temporal data. The input consisted exclusively of the raw time series, 
without additional metadata, in order to allow a direct comparison of the two distance metrics 
on the raw time series. A logarithmic transformation was first applied to reduce skewness and 
stabilize variance. For the Euclidean KNN, a global Z-score normalization was then performed 
across samples so that each time step contributed equally to the distance computation, 
while for the DTW-based KNN a per-series mean–variance normalization was applied to remove 
amplitude effects and focus the comparison on temporal shape rather than absolute scale. 
The dataset was split into stratified train and test sets (80/20), and \texttt{GridSearchCV} 
was used on the Euclidean KNN to optimize the number of neighbors setting as scoring \texttt{balanced\_accuracy}.
The best number of neighbors was found to be 9.

Using this configuration, the Euclidean KNN achieved an overall accuracy of approximately 
$0.46$, with a macro-averaged F1-score of $0.37$, indicating a moderate classification 
performance in a multi-class and partially imbalanced setting. The classification report
and the confusion matrix reveal
that the classifier is most effective in identifying the \textit{High} and \textit{Medium} 
rating categories, which also correspond to the most represented classes in the dataset. 
In particular, the \textit{High} class exhibits a relatively high recall (0.71),
indicating that highly rated items exhibit more distinctive temporal patterns.
In contrast, the \textit{Low} and \textit{Medium High} categories show substantially lower 
recall values, confirming that these categories are harder to separate and are often confused 
with adjacent rating levels. This behavior is consistent with the ordinal nature of the target variable, 
where errors tend to occur between semantically close categories rather than across 
distant ones.



The DTW-based KNN was evaluated using the same number of neighbors and a Sakoe–Chiba 
constraint with radius 7, chosen considering the observed seasonality of approximately 14 
time steps while preventing excessive temporal warping. The DTW classifier achieved a 
slightly lower accuracy of about $0.42$ and a macro F1-score of $0.35$, with performance trends similar to the Euclidean 
case, as shown in Figure~\ref{fig:cm_knn}. While DTW provides a more flexible alignment 
of temporal patterns, its advantage appears limited in this setting, likely due to the 
fact that, maybe, rating categories are more strongly associated with overall trend and magnitude 
patterns rather than fine-grained temporal misalignments. 


% Overall, the comparison suggests that, for this task, Euclidean KNN on globally normalized 
% time series offers a competitive and computationally simpler baseline, while DTW does not 
% yield a substantial improvement despite its higher computational cost.

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plotsss/cm_knn_euclidean.png}
        \caption{KNN with Euclidean distance}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plotsss/cm_knn_dtw.png}
        \caption{KNN with DTW distance}
    \end{subfigure}
    \caption{Confusion matrices for the KNN classifiers using Euclidean and DTW distances.}
    \label{fig:cm_knn}
\end{figure}


